{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efee1482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "import pandas as pd\n",
    "from datetime import datetime, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9be14a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"createDataFrameOverview2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a39ae3a",
   "metadata": {},
   "source": [
    "# Creación de DataFrame\n",
    "\n",
    "Se puede crear un DataFrame de PySpark mediante pyspark.sql.SparkSession.createDataFrame, generalmente pasando una lista de listas, tuplas, diccionarios y pyspark.sql.Rows, un DataFrame de pandas y un RDD que consta de dicha lista.\n",
    "\n",
    "pyspark.sql.SparkSession.createDataFrame toma el argumento de schema para especificar el esquema del DataFrame. Cuando se omite, PySpark infiere el esquema correspondiente tomando una muestra de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae90dc8f",
   "metadata": {},
   "source": [
    "### Crear un DataFrame a partir de una lista de filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a825538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n",
       "<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n",
       "<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-02-01 12:00:00</td></tr>\n",
       "<tr><td>4</td><td>5.0</td><td>string3</td><td>2000-03-01</td><td>2000-03-01 12:00:00</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---+-------+----------+-------------------+\n",
       "|  a|  b|      c|         d|                  e|\n",
       "+---+---+-------+----------+-------------------+\n",
       "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
       "|  2|3.0|string2|2000-02-01|2000-02-01 12:00:00|\n",
       "|  4|5.0|string3|2000-03-01|2000-03-01 12:00:00|\n",
       "+---+---+-------+----------+-------------------+"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 2, 1, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 3, 1, 12, 0))\n",
    "])\n",
    "\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae1d945b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-02-01 12:00:00|\n",
      "|  4|5.0|string3|2000-03-01|2000-03-01 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1.show()\n",
    "df_1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67344a78",
   "metadata": {},
   "source": [
    "### Crear un DataFrame con un esquema explícito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a68a100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n",
       "<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n",
       "<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-02-01 12:00:00</td></tr>\n",
       "<tr><td>4</td><td>5.0</td><td>string3</td><td>2000-03-01</td><td>2000-03-01 12:00:00</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---+-------+----------+-------------------+\n",
       "|  a|  b|      c|         d|                  e|\n",
       "+---+---+-------+----------+-------------------+\n",
       "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
       "|  2|3.0|string2|2000-02-01|2000-02-01 12:00:00|\n",
       "|  4|5.0|string3|2000-03-01|2000-03-01 12:00:00|\n",
       "+---+---+-------+----------+-------------------+"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 2, 1, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 3, 1, 12, 0))\n",
    "], schema='a int, b double, c string, d date, e timestamp')\n",
    "\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eaf147d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-02-01 12:00:00|\n",
      "|  4|5.0|string3|2000-03-01|2000-03-01 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- a: integer (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.show()\n",
    "df_2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb8427f",
   "metadata": {},
   "source": [
    "### Crear un DataFrame a partir de un DataFrame de pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6a2287c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n",
       "<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n",
       "<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02 12:00:00</td></tr>\n",
       "<tr><td>3</td><td>4.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03 12:00:00</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---+-------+----------+-------------------+\n",
       "|  a|  b|      c|         d|                  e|\n",
       "+---+---+-------+----------+-------------------+\n",
       "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
       "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
       "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
       "+---+---+-------+----------+-------------------+"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [2., 3., 4.],\n",
    "    'c': ['string1', 'string2', 'string3'],\n",
    "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
    "})\n",
    "\n",
    "df_3 = spark.createDataFrame(pandas_df)\n",
    "\n",
    "df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee45c9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_3.show()\n",
    "\n",
    "df_3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a87767",
   "metadata": {},
   "source": [
    "# Visualización de datos\n",
    "\n",
    "Las filas superiores de un DataFrame se pueden visualizar utilizando DataFrame.show()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "330440f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13059e4",
   "metadata": {},
   "source": [
    "Como alternativa, puede habilitar la configuración spark.sql.repl.eagerEval.enabled para la evaluación diligente de PySpark DataFrame en notebooks como Jupyter. La cantidad de filas que se mostrarán se puede controlar mediante la configuración spark.sql.repl.eagerEval.maxNumRows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6a55871",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c010f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n",
       "<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n",
       "<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-02-01 12:00:00</td></tr>\n",
       "<tr><td>4</td><td>5.0</td><td>string3</td><td>2000-03-01</td><td>2000-03-01 12:00:00</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f3f06e",
   "metadata": {},
   "source": [
    "Las filas también se pueden mostrar en forma vertical. Esto resulta útil cuando las filas son demasiado largas para mostrarlas en forma horizontal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb118077",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " a   | 1                   \n",
      " b   | 2.0                 \n",
      " c   | string1             \n",
      " d   | 2000-01-01          \n",
      " e   | 2000-01-01 12:00:00 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6346a12b",
   "metadata": {},
   "source": [
    "Puede ver el esquema y los nombres de las columnas del DataFrame de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a3b63e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef6465bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168e2da0",
   "metadata": {},
   "source": [
    "Mostrar el resumen del DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ce26842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-------+\n",
      "|summary|                 a|                 b|      c|\n",
      "+-------+------------------+------------------+-------+\n",
      "|  count|                 3|                 3|      3|\n",
      "|   mean|2.3333333333333335|3.3333333333333335|   NULL|\n",
      "| stddev|1.5275252316519468|1.5275252316519468|   NULL|\n",
      "|    min|                 1|               2.0|string1|\n",
      "|    max|                 4|               5.0|string3|\n",
      "+-------+------------------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1.select(\"a\", \"b\", \"c\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5275e32e",
   "metadata": {},
   "source": [
    "DataFrame.collect() recopila los datos distribuidos en el lado del controlador como datos locales en Python. Tenga en cuenta que esto puede generar un error de falta de memoria cuando el conjunto de datos es demasiado grande para caber en el lado del controlador, ya que recopila todos los datos de los ejecutores en el lado del controlador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6bbb418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n",
       " Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 2, 1, 12, 0)),\n",
       " Row(a=4, b=5.0, c='string3', d=datetime.date(2000, 3, 1), e=datetime.datetime(2000, 3, 1, 12, 0))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bd8d85",
   "metadata": {},
   "source": [
    "Para evitar lanzar una excepción por falta de memoria, utilice DataFrame.take() o DataFrame.tail()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af44075a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n",
       " Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 2, 1, 12, 0)),\n",
       " Row(a=4, b=5.0, c='string3', d=datetime.date(2000, 3, 1), e=datetime.datetime(2000, 3, 1, 12, 0))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e0d1043",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n",
       " Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 2, 1, 12, 0)),\n",
       " Row(a=4, b=5.0, c='string3', d=datetime.date(2000, 3, 1), e=datetime.datetime(2000, 3, 1, 12, 0))]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2cd72f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n",
       " Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 2, 1, 12, 0)),\n",
       " Row(a=4, b=5.0, c='string3', d=datetime.date(2000, 3, 1), e=datetime.datetime(2000, 3, 1, 12, 0))]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed303ab9",
   "metadata": {},
   "source": [
    "PySpark DataFrame también permite la conversión a un DataFrame de pandas para aprovechar la API de pandas. Tenga en cuenta que toPandas también recopila todos los datos en el lado del controlador, lo que puede provocar fácilmente un error de falta de memoria cuando los datos son demasiado grandes para caber en el lado del controlador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ffb1348b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>string1</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2000-01-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>string2</td>\n",
       "      <td>2000-02-01</td>\n",
       "      <td>2000-02-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>string3</td>\n",
       "      <td>2000-03-01</td>\n",
       "      <td>2000-03-01 12:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a    b        c           d                   e\n",
       "0  1  2.0  string1  2000-01-01 2000-01-01 12:00:00\n",
       "1  2  3.0  string2  2000-02-01 2000-02-01 12:00:00\n",
       "2  4  5.0  string3  2000-03-01 2000-03-01 12:00:00"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
